{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"colab":{"name":"Group8 - FacialExpressionCompetition"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATASETS\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S R\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\nDATASET_MAPPING = 'bda-2023-facial-expressions:http%3A%2F%2Flocalhost%2Flocalgs%2Fv1%2Fdownload%2Fkaggle-competitions-data%2Fkaggle-v2%2F60511%2F6530139%2Fbundle%2Farchive.zip'\nKAGGLE_INPUT_PATH = '/home/kaggle/input'\nKAGGLE_INPUT_SYMLINK = '/kaggle'\n\nsystem(paste0('sudo mkdir -p -- ', KAGGLE_INPUT_PATH), intern=TRUE)\nsystem(paste0('sudo chmod 777 ', KAGGLE_INPUT_PATH), intern=TRUE)\nsystem(paste0('sudo ln -sfn ', KAGGLE_INPUT_PATH, ' ../'), intern=TRUE)\nsystem(paste0('sudo mkdir -p -- ', KAGGLE_INPUT_SYMLINK))\nsystem(paste0('sudo ln -sfn ', KAGGLE_INPUT_PATH, ' ', KAGGLE_INPUT_SYMLINK))\n\ndatasets_mappings = strsplit(DATASET_MAPPING, ',')[[1]]\nfor (dataset_mapping in datasets_mappings) {\n    path_and_url = strsplit(dataset_mapping, ':')\n    directory = path_and_url[[1]][1]\n    download_url = URLdecode(path_and_url[[1]][2])\n    destination_path = file.path(KAGGLE_INPUT_PATH, directory)\n    temp = tempfile(fileext = '.zip')\n    download.file(download_url, temp)\n    unzip(temp, overwrite = TRUE, exdir = destination_path)\n    unlink(temp)\n    print(paste0('Downloaded and unzipped: ', directory))\n}\n","metadata":{},"cell_type":"code","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# <center> **FACIAL EMOTION RECOGNITION**\n    \n# 1. Introduction\n\nIn this competition, we will build a classifier algorithm that categorizes facial expressions of humans in gray-scale portraits into one of four emotion cagetories: anger, happiness, disgust, sadness \n    \n    \nBefore we get started, we will answer the following questions:\n    \n**Where do the data come from, and to which population will results generalize?**\n    \n> The dataset is called CKPlus. It is a  widely used facial expression database used for research in the field of facial expression analysis and computer vision. (S. Li, W. Deng, Deep facial expression recognition: a survey. IEEE Trans. Affect. Comput. 13, 1195â€“1215 (2020)). It consists of both posed and spontaneous expressions from a total of 123 subjects with diverse ethnic backgrounds aged between 18-50 years. The pictures were taken under controlled lighting conditions with subjects seated in front of a camera. Researchers coded expressed emotion , the intensity of the emotion, and the location of facial landmarks. They included seven different emotions: Anger, Contempt, Disgust, Fear, Happiness, Sadness, Surprise. \n>\n> For our project, we only used a subset of the CL+ database. We only distinguished between four different emotions and did not include its intensity. Additionaly, some of the images are repeated, but then shifted, rotated, or both to improve the algorithm. \n>    \n> While the datset did include subjects of different ethnicities and age, the generalizability of our results will still be limited. As metioned, the pictures were taken under controlled conditions with consistent and good lighting. This means that our algorithm will probably have difficulties recognizing facial expressions in more \"normal\", day-to-day pictures. In addition to that, the datapoints are not independent of each other as multiple pictures were taken of each participant. Independence, however, is an important assumption of many algorithms and statistical tests.     \n\n**What are candidate machine learning methods?**\n\n> There is a wide variety of possible algorithms to choose from. These algorithms differ in their levels of flexibility and bias. Possible algorithms sorted from least to most flexible are: \n> Linear Discrimminant Analysis, multinomial logistic regression, Ridge Regression, Lasso Regression, , K-nearest Neighbors, Random Forestn, Support Vector \n> \n> The more flexible a model is, the more it is driven by the training data and will probably achieve a good model fit. The \"danger\" of highly flexible models is that they *overfit* the training data which would mean that they do not predict future observations well, which is our goal. This is why we chose less flexible (aka more biased) models that make stronger assumptions about the underlying distribution. This might result it a worse model fit, but will likely predict future cases better. \n> This is why we decided on fitting ...\n\n**What is the Bayes' error bound?**\n\n> The Bayes' error bound represents a fundamental theoretical limit on the classification error rate (aka the minimym accuracy) that any algorithm should achieve. Previous research has determined the accuracy of humans when as 67% for disgust, 62% for anger, 80% for happy, 70% for sad. Human accuracy has also been tested for the CK+ database specifically which lead to the confusion matrix shown below and an overall accuracy of 74.4%. <br>\n>\n>|      | Anger | Disgust | Happy | Sadness |\n>|------|-------|---------|-------|---------|\n>| Anger   | 23   | 9       | 1     | 0       |\n>| Disgust | 6    | 35      | 4     | 2       |\n>| Happy   | 4    | 3       | 66    | 0       |\n>| Sad     | 9    | 7       | 1     | 10      |\n>\n> For Machine Learning models the Bayes Error Bound is discussed in: https://www.hindawi.com/journals/cin/2022/8032673/ The reported accuracies for emotion recognition through facial expression range in the experiment have a training accuracy of 100% and a test accuracy of up to 90% for SVM and RF and up to 95% for KNN.\n    \n    \n# 2. Data\n    \n    \n## 2.1 Preparation\n    \nFirst, we loaded all necessary libraries, set the working directory and imported all image files. \n    ","metadata":{}},{"cell_type":"code","source":"## loading libraries\n\nlibrary(tidyverse) \nlibrary(png) \nlibrary(caret)\nsuppressMessages(library(tidytext))\nsuppressMessages(library(caret))\nsuppressMessages(library(glmnet))\nsuppressMessages(library(ggplot2))\nsuppressMessages(library(gridExtra))\n\n## Reading in files\n\nlist.files(path = \"../input/\")\n\n\n# Show availabe directories\ndirs = dir(\"../input\", pattern=\"[^g]$\", recursive=TRUE, include.dirs = TRUE, full.names = TRUE)\n# dirs\n\n# Get all image files: file names ending \".png\" \nanger   = dir(grep(\"anger\",   dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\ndisgust = dir(grep(\"disgust\", dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\nhappy   = dir(grep(\"happy\",   dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\nsad     = dir(grep(\"sad\",     dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)\ntest_im = dir(grep(\"test\",    dirs, value = TRUE), pattern = \"png$\", full.names = TRUE)","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2023-10-17T14:30:10.257692Z","iopub.execute_input":"2023-10-17T14:30:10.259437Z","iopub.status.idle":"2023-10-17T14:30:15.17037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Loading the data\n\nNext, we loaded the data and created a function to visualize some of the pictures to make sure that everything worked. \n\nEach imagine is stored in a vector of pixel intensities. Another vector contains the emotion labels for each of the images. In order to save on RAM, the images for this competition are...\n\n* ... gray scale, so we need only one color channel\n* ... are only 48 by 48 pixels\n","metadata":{}},{"cell_type":"code","source":"# Combine all filenames into a single vector\ntrain_image_files = c(anger, happy, sad, disgust)\n\n# Read in the images as pixel values (discarding color channels)\nX = sapply(train_image_files, function(nm) c(readPNG(nm)[,,1])) %>% t() \ny = c(rep(\"anger\", length(anger)), rep(\"happy\", length(happy)), rep(\"sad\", length(sad)), rep(\"disgust\", length(disgust)))\n\nX_test = sapply(test_im, function(nm) c(readPNG(nm)[,,1])) %>% t() \n\n\n# Change row and column names of X to something more managable (caret::train requires column names)\nrownames(X)      = gsub(\".+train/\", \"\", rownames(X))\nrownames(X_test) = gsub(\".+test/\",  \"\", rownames(X_test))\n\ncolnames(X) = colnames(X_test) = paste(\"p\",1:ncol(X), sep=\"\")\n\n# Check result (are X, X_test, and y what we expect)\n# X[1:6,20:23] %>% print\n# table(y)\n# X_test[1:6,20:23] %>% print\n                \n                \n# Visualization utility function\nas_image = function(x, nr=sqrt(length(x))) {opar=par(mar=rep(0,4)); on.exit(par(opar)); image(t(matrix(x,nr))[,nr:1], col = gray(0:255/255),axes=F)}","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:15.173252Z","iopub.execute_input":"2023-10-17T14:30:15.17476Z","iopub.status.idle":"2023-10-17T14:30:19.664392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Creating a Test Subset\n\nLastly, we split the data into a test and training set to evaluate our model's performance using a confusion matrix. ","metadata":{}},{"cell_type":"code","source":"set.seed(123)\n\n# Create random partition of the data\ntrain_val = caret::createDataPartition(y, p = 0.8, list = FALSE)\n\n# Define training & validation set\ntrain_X = X[train_val, ]\n#train_X = X[sample(train_val, 500),] # REMOVE THIS FOR CREATING AN ACTUAL SUBMISSION F\ntrain_y = y[train_val]\n\ntest_X = X[-train_val, ]\ntest_y = y[-train_val]\n\nX <- rbind(train_X, test_X) # merge X_train and X_test for the moment to compute features","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:19.667032Z","iopub.execute_input":"2023-10-17T14:30:19.668432Z","iopub.status.idle":"2023-10-17T14:30:20.253968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot some of the pictures\nlayout(matrix(1:4, nrow = 2))\n\npar(mar = c(2, 2, 2, 2))\nas_image(X[41,])\nmtext(train_y[41])\n\npar(mar = c(2, 2, 2, 2))\nas_image(X_test[41,])\nmtext(test_y[41])\n\npar(mar = c(2, 2, 2, 2))\nas_image(X[499,])\nmtext(train_y[499])\n\npar(mar = c(2, 2, 2, 2))\nas_image(X_test[499,])\nmtext(test_y[499])","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:20.256598Z","iopub.execute_input":"2023-10-17T14:30:20.257999Z","iopub.status.idle":"2023-10-17T14:30:20.434608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Note: It looks like some of the picture labels are wrong. We could not figure out where we went wrong because we did not change the code we were given._\n\n# 3. Feature Extraction\n\nTo classify the pictures we need to extract features from them. They need to be invariant to transformations like flipping, skewing, ... to accomodate the wide variety in pictures and faces. To do so will not only use the \"raw\" pixels we extracted above, but also so-called \"edges\". Edges represent areas where the pixel values change abruptly. This gives an outline of the pictures based on significant changes in intensity, color, or texture of the image. \n\n## 3.1 Function for extracting summary statistics\n\nTo make the working process easier, we wrote a function that automatically extracts basic summary statistics like mean, variance, standard deviation, ... . Additionally, we defined some functions ourselves. \n\n### 3.1.1 Entropy","metadata":{}},{"cell_type":"code","source":"entropy <- function(X, nbreaks = 30) {\n    r <- range(X)\n    x_binned <- findInterval(X, seq(r[1], r[2], length.out = nbreaks))\n    h <- tabulate(x_binned, nbins = nbreaks) # fast histogram\n    p <- h / sum(h)\n    -sum(p[p > 0] * log(p[p > 0]))\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:20.437328Z","iopub.execute_input":"2023-10-17T14:30:20.43883Z","iopub.status.idle":"2023-10-17T14:30:20.452023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2 Combining it all in one function","metadata":{}},{"cell_type":"code","source":"stats <- function(input_df, suffix) {\n    \n  # Calculate mean, variance, standard deviation, ... per row\n    meansX <- rowMeans(input_df)\n    # meansY <- colMeans(input_df)\n    variancesX <- apply(input_df, 1, var)\n    # variancesY <- apply(input_df, 2, var)\n    std_dev <- apply(input_df, 1, sd)\n    mins <- apply(input_df, 1, min)\n    maxs <- apply(input_df, 1, max)\n    medians <- apply(input_df, 1, median)\n    entropies <- apply(input_df, 1, entropy)\n    \n\n  # Create a new dataframe with modified column names\n  result_df <- data.frame(\n    MeanX = meansX,\n    #  MeanY = meansY,\n    VarianceX = variancesX,\n    # VarianceY = variancesY,\n    StandardDeviation = std_dev,\n    Minima = mins,\n    Maxima = maxs,\n    Width = maxs - mins, \n    Medians = medians,\n    Entropy = entropies\n  )\n\n  # Add the specified suffix to column names\n  colnames(result_df) <- paste0(colnames(result_df), \"_\", suffix)\n\n  return(result_df)\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:20.454677Z","iopub.execute_input":"2023-10-17T14:30:20.456115Z","iopub.status.idle":"2023-10-17T14:30:20.469762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Raw Pixel Analysis\n\nBefore analyzing edges, we applied some basic summary statistics to the raw pixels \n","metadata":{}},{"cell_type":"code","source":"train_stats_features_pixels <- stats(train_X,\"pixel\")\ntest_stats_features_pixels <- stats(test_X,\"pixel\")\nhead(train_stats_features_pixels, 3)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:20.472368Z","iopub.execute_input":"2023-10-17T14:30:20.473789Z","iopub.status.idle":"2023-10-17T14:30:22.974848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Edge Analysis\n\n### 3.3.1 Extracting Edges","metadata":{}},{"cell_type":"code","source":"# Define the threshold\nthreshold <- 0.0625\n\n# Function to perform edge detection\nedge_detection <- function(image) {\n    \n  h_edge <- image[-1,] - image[-nrow(image),]\n  v_edge <- image[,-1] - image[,-ncol(image)]\n  d_edge <- h_edge[,-1] - h_edge[,-ncol(h_edge)]\n\n  # Apply the threshold\n  h_edgeT <- h_edge < threshold\n  v_edgeT <- v_edge < threshold\n  d_edgeT <- d_edge < threshold / 2\n\n  # Return the results as a list\n  return(list(horizontal = h_edge, vertical = v_edge, diagonal = d_edge, \n              horizontalThreshold = h_edgeT, verticalThreshold = v_edgeT, \n              diagonalThreshold = d_edgeT))\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:22.977532Z","iopub.execute_input":"2023-10-17T14:30:22.97924Z","iopub.status.idle":"2023-10-17T14:30:22.997874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.2 Raw edges","metadata":{}},{"cell_type":"code","source":"# Flatten and convert list of matrices from edge_detection into a numeric vector\nflatten_edges <- function(edge_list) {\n  # Convert each matrix in the list into a vector and concatenate them all into a single vector\n  return(c(as.vector(edge_list$horizontal), \n           as.vector(edge_list$vertical),\n           as.vector(edge_list$diagonal)))\n}\n\n# Extract Edge Features\ntrain_edge_features <- lapply(1:nrow(train_X), function(i) {\n  im <- matrix(train_X[i,], 48) \n  edges <- edge_detection(im)\n  return(flatten_edges(edges))\n})\ntest_edge_features <- lapply(1:nrow(test_X), function(i) {\n  im <- matrix(test_X[i,], 48) \n  edges <- edge_detection(im)\n  return(flatten_edges(edges))\n})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:23.000996Z","iopub.execute_input":"2023-10-17T14:30:23.002524Z","iopub.status.idle":"2023-10-17T14:30:23.7331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to visualize the original and edge images\nvisualize_edge_features <- function(original_matrix, edge_features, index) {\n  # Extract original image and reshape it to a 48x48 matrix\n  original_image <- matrix(original_matrix[index,], 48)\n  \n  # Extract and reshape edge features to their respective image shapes\n  h_edge_image <- matrix(edge_features[[index]][1:(48*47)], 48, 47, byrow = TRUE)\n  v_edge_image <- matrix(edge_features[[index]][(48*47 + 1):(48*47 + 48*47)], 48, 47, byrow = TRUE)\n  d_edge_image <- matrix(edge_features[[index]][(2*48*47 + 1):(2*48*47 + 47*47)], 47, 47, byrow = TRUE)\n\n  # Plotting\n  p1 <- ggplot() +\n    geom_raster(data = NULL, aes(x = x, y = y, fill = original), interpolate = TRUE) +\n    scale_fill_gradient(low = \"black\", high = \"white\") +\n    ggtitle(\"Original Image\") +\n    theme_void() + theme(legend.position = \"none\") +\n    coord_fixed(ratio = 1)\n  p1$data <- expand.grid(x = 1:48, y = 1:48)\n  p1$data$original <- as.vector(t(original_image))\n\n  p2 <- ggplot() +\n    geom_raster(data = NULL, aes(x = x, y = y, fill = h_edge), interpolate = TRUE) +\n    scale_fill_gradient(low = \"black\", high = \"white\") +\n    ggtitle(\"Horizontal Edges\") +\n    theme_void() + theme(legend.position = \"none\") +\n    coord_fixed(ratio = 1)\n  p2$data <- expand.grid(x = 1:48, y = 1:47)\n  p2$data$h_edge <- as.vector(t(h_edge_image))\n\n  p3 <- ggplot() +\n    geom_raster(data = NULL, aes(x = x, y = y, fill = v_edge), interpolate = TRUE) +\n    scale_fill_gradient(low = \"black\", high = \"white\") +\n    ggtitle(\"Vertical Edges\") +\n    theme_void() + theme(legend.position = \"none\") +\n    coord_fixed(ratio = 1)\n  p3$data <- expand.grid(x = 1:47, y = 1:48)\n  p3$data$v_edge <- as.vector(t(v_edge_image))\n\n  p4 <- ggplot() +\n    geom_raster(data = NULL, aes(x = x, y = y, fill = d_edge), interpolate = TRUE) +\n    scale_fill_gradient(low = \"black\", high = \"white\") +\n    ggtitle(\"Diagonal Edges\") +\n    theme_void() + theme(legend.position = \"none\") +\n    coord_fixed(ratio = 1)\n  p4$data <- expand.grid(x = 1:47, y = 1:47)\n  p4$data$d_edge <- as.vector(t(d_edge_image))\n\n  # Arrange the plots in a grid\n  grid.arrange(p1, p2, p3, p4, ncol = 2)\n}\n\n# Use the function to visualize the 1st image and its edge features\nvisualize_edge_features(train_X, train_edge_features, 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:23.73608Z","iopub.execute_input":"2023-10-17T14:30:23.737699Z","iopub.status.idle":"2023-10-17T14:30:24.829971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the list of edge features into a matrix\ntrain_edge_features_matrix <- do.call(rbind, train_edge_features)\ntest_edge_features_matrix <- do.call(rbind, test_edge_features)\n\n# Extract Statistical Features of edges\ntrain_stats_features_edges <- stats(train_edge_features_matrix, \"edge\")\ntest_stats_features_edges <- stats(test_edge_features_matrix, \"edge\")\n\nhead(train_stats_features_edges, 3)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:24.832563Z","iopub.execute_input":"2023-10-17T14:30:24.833917Z","iopub.status.idle":"2023-10-17T14:30:30.882681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.3 Thresholded edges","metadata":{}},{"cell_type":"code","source":"# Apply the threshold to the matrix\ntrain_edge_thresholded_matrix <- ifelse(train_edge_features_matrix < threshold, 0, 1)\ntest_edge_thresholded_matrix <- ifelse(test_edge_features_matrix < threshold, 0, 1)\n\ntrain_stats_thresholded_edges <- stats(train_edge_thresholded_matrix, \"thresholded_edges\")\ntest_stats_thresholded_edges <- stats(test_edge_thresholded_matrix, \"thresholded_edges\")\n\nhead(train_stats_thresholded_edges, 3)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:30.885682Z","iopub.execute_input":"2023-10-17T14:30:30.887097Z","iopub.status.idle":"2023-10-17T14:30:35.987819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Creating and Cleaning Final Dataframe\n\n## 4.1 Combining everything in one dataframe","metadata":{}},{"cell_type":"code","source":"# Combine raw pixels, edge features, and their statistical features\ntrain_X_combined <- cbind(train_edge_features_matrix, train_edge_thresholded_matrix, \n                          train_stats_features_pixels, train_stats_features_edges, train_stats_thresholded_edges)\n\ntest_X_combined <- cbind(test_edge_features_matrix, test_edge_thresholded_matrix,\n                         test_stats_features_pixels, test_stats_features_edges, test_stats_thresholded_edges)\n\n\n# Normalize the combined features\ntrain_X_normalized <- scale(train_X_combined)\ntest_X_normalized <- scale(test_X_combined)\n\n# Now train_X_normalized and test_X_normalized can be used for model training and testing\n\nhead(train_X_normalized, 3)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:35.990658Z","iopub.execute_input":"2023-10-17T14:30:35.99212Z","iopub.status.idle":"2023-10-17T14:30:41.626473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1 Removing near Zero-variance pixels\n\nPixels were removed due to near zero variance. For recognizing emotions through facial expressions the pixels that differ between emotions are limited. Many pixels, the ones that represent the outer part of the face, do not change by expressing an emotions. Those pixels can be removed.","metadata":{}},{"cell_type":"code","source":"variance_threshold <- 0.045\nvariances <- apply(X, 2, var)\n\n# Filter the columns based on the variance threshold\nfiltered_X <- X[, variances >= variance_threshold]\n\n# Calculate the number of removed columns\nnum_removed_columns <- ncol(X) - ncol(filtered_X)\n\n# Display the number of removed columns and the head of the original matrix\ncat(\"Number of removed columns:\", num_removed_columns, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:41.629267Z","iopub.execute_input":"2023-10-17T14:30:41.630736Z","iopub.status.idle":"2023-10-17T14:30:41.907221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.1 plotting removed pixels","metadata":{}},{"cell_type":"code","source":"# 1. Create a binary matrix of the same size as your image.\npixel_retention_matrix <- matrix(1, nrow=48, ncol=48)\n\n# 2. Set the removed pixels to 0.\nremoved_pixel_indices <- which(variances < variance_threshold)\npixel_retention_matrix[removed_pixel_indices] <- 0\n\n# 3. Plot the binary matrix.\nlibrary(ggplot2)\n\npixel_df <- data.frame(\n  x = rep(1:48, each=48),\n  y = rep(1:48, times=48),\n  retained = as.vector(pixel_retention_matrix)\n)\n\nggplot(pixel_df, aes(x=x, y=y, fill=factor(retained))) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"red\", \"blue\"), \n                    name = \"Pixel Status\",\n                    breaks = c(0, 1),\n                    labels = c(\"Removed\", \"Retained\")) +\n  coord_fixed(ratio = 1) +\n  theme_void() +\n  labs(title = \"Pixels Removed Based on Variance Threshold\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:41.910076Z","iopub.execute_input":"2023-10-17T14:30:41.91207Z","iopub.status.idle":"2023-10-17T14:30:42.212192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify common columns\ncommon_columns <- intersect(colnames(X), colnames(filtered_X))\n\n# Subset X to keep only common columns\ntrain_X <- train_X[, common_columns]\ntest_X <- test_X[, common_columns]\n\nncol(train_X) == ncol(filtered_X)\nncol(test_X) == ncol(filtered_X)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:42.215413Z","iopub.execute_input":"2023-10-17T14:30:42.21723Z","iopub.status.idle":"2023-10-17T14:30:42.262847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding non-zero pixels to dataframe\ntrain_X_combined <- cbind(train_X_combined, train_X)\ntest_X_combined <- cbind(test_X_combined, test_X)\n\nhead(train_X_combined, 3)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:42.267143Z","iopub.execute_input":"2023-10-17T14:30:42.268843Z","iopub.status.idle":"2023-10-17T14:30:48.67685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeing up some RAM\n# List the names of the objects you want to keep\nobjects_to_keep <- c(\"train_X_combined\", \"test_X_combined\", \"train_y\", \"test_y\")\n\n# List all objects in the current environment\nall_objects <- ls()\n\n# Remove all objects except the ones in objects_to_keep\nobjects_to_remove <- setdiff(all_objects, objects_to_keep)\nrm(list = objects_to_remove)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:48.680997Z","iopub.execute_input":"2023-10-17T14:30:48.682704Z","iopub.status.idle":"2023-10-17T14:30:48.705835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 High Correlations","metadata":{}},{"cell_type":"code","source":"# high_corr <- findCorrelation(cor(train_X_combined), .95)\n\n# cat(\"Deletes the following:\", ifelse(!is.null(high_corr), paste(names(train_X_combined)[high_corr], collapse = \", \"), \"Deletes nothing\"))\n\n# CleanedFinalDf <- train_X_combined %>%\n#  select(-all_of(high_corr)) \n\n# head(CleanedFinalDf, 6)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:48.710293Z","iopub.execute_input":"2023-10-17T14:30:48.721152Z","iopub.status.idle":"2023-10-17T14:30:48.735436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4 Normalizing Data\n\nFor some models the data needs to be normalized which we do in this very last step. ","metadata":{}},{"cell_type":"code","source":"# Normalize the combined features\ntrain_X_normalized <- scale(train_X_combined)\ntest_X_normalized <- scale(test_X_combined)\n\nhead(train_X_normalized, 3)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:48.739877Z","iopub.execute_input":"2023-10-17T14:30:48.741988Z","iopub.status.idle":"2023-10-17T14:30:54.090597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model Fitting\n\n\n## 5.1 Classification tree\n\nWe consider classification trees and random forests. Random forests are probably the least susceptible to overtraining and is considered one of the best \"off the shelf\" machine learning algorithms in the sense that they require little expertise in application, and easily perform well without tuning.\n\nWe fit a classification tree, using the pixel based approach. ","metadata":{}},{"cell_type":"code","source":"suppressMessages(require(caret))\n\n## Use multiple cores whenever possible\ndoMC::registerDoMC(cores = 4) \n\n## Fit a CART using 5-fold cross-validation to tune the complexity parameter\nset.seed(2023) # for repeatability (generally pick the seed randomly)\ntrCntrl = trainControl('cv', 5, allowParallel = TRUE)\ntt <- Sys.time()\nfit_tree = train(x=train_X_normalized, y=train_y, method='rpart', trControl = trCntrl, tuneGrid = data.frame(cp=.02))\nfit_tree\n\n(dur <- Sys.time() - tt)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T14:30:54.093896Z","iopub.execute_input":"2023-10-17T14:30:54.095398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Graphical visualization of the decision tree\noptions(repr.plot.width=14, repr.plot.height=8)\nplot(fit_tree$final, compress=TRUE, uniform=TRUE, margin=0.05, branch=.75); \ntext(fit_tree$final, cex=0.8, all=TRUE, use.n=TRUE)\n\n## Textual visualization of the decision tree\nfit_tree$finalModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cross validated accuracy estimate is around 56%.","metadata":{}},{"cell_type":"code","source":"## Check performance on the normalized training set\npred_tree = predict(fit_tree, train_X_normalized, type='raw') \nconfusion_tree <- confusionMatrix(pred_tree, factor(train_y))\n\n\n# Overall accuracy\naccuracy_tree <- confusion_tree$overall['Accuracy']\n\n# Print them\nprint(confusion_tree$table)\nprint(confusion_tree$byClass[,1:2])\nprint(paste(\"Overall Accuracy: \", round(accuracy_tree, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Ridge (Multinomial Regression with Ridge)","metadata":{}},{"cell_type":"code","source":"# Speed up tuning by using all 4 CPU cores\ndoMC::registerDoMC(cores = 4)\n\n#Performing ridge regression\nfit_ridge <- glmnet::cv.glmnet(as.matrix(train_X_normalized),\n                       factor(train_y),\n                       family = \"multinomial\",\n                       nfolds = 5,\n                       alpha = 0,\n                       type.measure = \"class\",\n                       standardize = TRUE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performance evaluation CV Ridge\npred_ridge = predict(fit_ridge, \n                     as.matrix(test_X_normalized), \n                     s = 'lambda.min', \n                     type = 'class') %>% \nas.factor()\n\n# Assessing Accuracy in Confusion-Matrix\nconfusion_ridge <- caret::confusionMatrix(pred_ridge,factor(test_y))\n\n# Overall accuracy\naccuracy_ridge <- confusion_ridge$overall['Accuracy']\n\n# Print them\nprint(confusion_ridge$table)\nprint(confusion_ridge$byClass[,1:2])\nprint(paste(\"Overall Accuracy: \", round(accuracy_ridge, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Lasso (Multinomial Regression with Lasso)","metadata":{}},{"cell_type":"code","source":"# Time how long the model-fitting takes\n# Speed up tuning by using all 4 CPU cores\ndoMC::registerDoMC(cores = 4)\n\n#Performing ridge regression\nfit_lasso <- glmnet::cv.glmnet(as.matrix(train_X_normalized), \n                       factor(train_y), \n                       family = \"multinomial\",\n                       nfolds = 5,\n                       alpha = 1, \n                       type.measure = \"class\",\n                       standardize = TRUE)\n\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\n\n# Plot misclassicication rate over the log of Lambda \nplot(fit_lasso)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performance evaluation CV Lasso\npred_lasso = predict(fit_lasso, \n                     as.matrix(test_X_normalized), \n                     s = 'lambda.min', \n                     type = 'class') %>% \nas.factor()\n\n# Assessing Accuracy in Confusion-Matrix\nconfusion_lasso = caret::confusionMatrix(pred_lasso,factor(test_y))\n\n# Overall accuracy\naccuracy_lasso <- confusion_lasso$overall['Accuracy']\n\n# Print them\nprint(confusion_lasso$table)\nprint(confusion_lasso$byClass[,1:2])\nprint(paste(\"Overall Accuracy: \", round(accuracy_lasso, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 Linear Discriminant Analysis (LDA)","metadata":{}},{"cell_type":"code","source":"#trcntr <- trainControl('cv', \n#                       number = 2, \n#                       p = 0.8)\n\n#fit_lda <- train(train_y ~ ., \n#                data = cbind(train_X_normalized, train_y), \n#                method = \"lda\", \n#                trControl = trcntr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the validation set\n#pred_lda <- predict(fit_lda, \n#                   test_X_normalized, \n#                   type='raw') \n\n# Assessing Accuracy in Confusion-Matrix\n#confusion_lda = caret::confusionMatrix(pred_lda,factor(test_y))\n\n# Overall accuracy\n#accuracy_lda <- confusion_lda$overall['Accuracy']\n\n# Print them\n#print(confusion_lda$table)\n#print(confusion_lda$byClass[,1:2])\n#print(paste(\"Overall Accuracy: \", round(accuracy_lda, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.5 Support Vector Machine (SVM)","metadata":{}},{"cell_type":"code","source":"# note. for fitting this model we use the NOT normalized data because data preprocessing is built in this function. \nfit_svm <- train(x = train_X_combined, \n                 y = train_y, \n                 method = \"svmRadial\", \n                 trControl = trCntrl, \n                 preProcess = c(\"center\",\"scale\"), \n                 tuneLength = 10)\n\n# Calculate predictions\npred_svm <- predict(fit_svm, test_X_combined)\n\n# Assessing Accuracy in Confusion-Matrix\nconfusion_svm = caret::confusionMatrix(pred_svm,factor(test_y))\n\n# Overall accuracy\naccuracy_svm <- confusion_svm$overall['Accuracy']\n\n# Print them\nprint(confusion_svm$table)\nprint(confusion_svm$byClass[,1:2])\nprint(paste(\"Overall Accuracy: \", round(accuracy_svm, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## e1071\n\n# fit model\ntune_svm <- e1071::svm(x = train_X_normalized,\n                       y = factor(train_y),\n                       kernel = 'radial',\n                       cost = 1,\n                       scale = TRUE)\n\n# Calculate predictions\npred_e1071 <- predict(tune_svm, test_X_normalized)\n\n# Compute confusion matrix\nCon_e1071 <- confusionMatrix(pred_e1071, factor(test_y))\n\n\n# Assessing Accuracy in Confusion-Matrix\nconfusion_e1017 = caret::confusionMatrix(pred_e1071,factor(test_y))\n\n# Overall accuracy\naccuracy_e1017 <- confusion_e1017$overall['Accuracy']\n\n# Print them\nprint(confusion_e1017$table)\nprint(confusion_e1017$byClass[,1:2])\nprint(paste(\"Overall Accuracy: \", round(accuracy_e1017, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Model Comparison\n\nThe last step was choosing the best out of the five models we fitted. <br>\n\n## 6.1 Bias-Variance Trade-off\nA general thing to consider during model selection between models with varying levels of flexibility is the bias-variance trade-off. The more flexible a model is, the less biased it is, but the more variance it has and vice versa. More biased models make stronger assumptions about the underlying data distribution. Because of this, they might fit the (training) data less well than less biased models if the assumed underlying distribution is incorrect. Models with high variance, often have the opposite problem: they might fit the (training) data very well but predict future cases less well because of overfitting.\n\nAs mentioned above, the different modelling algorithms we used are associated with different levels of flexibility. In contrast to previous competitions, the goal of this competition is to achieve a good prediction accuracy, instead of the correct classification of unknown test data. Therefore we do not see from using also more flexible models. \n\n## 6.2 Prediction Accuracy\n\nPrediction Accuracy refers to the proportion of correctly classified cases. Results showed that there is a \"clear winnder\", a \"clear looser\" and three medium models. The classification tree model performed a lot worse than all other models with an accuracy of 0.65. The support vector model performed the best with an accuracy of 0.95. ","metadata":{}},{"cell_type":"code","source":"FitMeasures <- data.frame(Model = c(\"Classification Tree\", \"Ridge\", \"Lasso\", \"Support Vector\", \"Tuned Support Vector\"), \n                          Accuracy = c(accuracy_tree, accuracy_ridge, accuracy_lasso, accuracy_svm, accuracy_e1017))\n\n# Create the ggplot bar plot\nggplot(FitMeasures, aes(x = Accuracy, y = Model)) +\n  geom_bar(stat = \"identity\", fill = ifelse(FitMeasures$Accuracy == max(FitMeasures$Accuracy), \"blue\", \"gray\")) +\n  geom_text(aes(label = round(Accuracy, 3)), hjust = -0.1) +\n  labs(title = \"Test Set Accuracy\", x = \"Accuracy\") +\n  xlim(0, 1.1) +\n  theme_minimal() +\n  coord_flip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Cohen's Kappa\n\nCohen's Kappa is a measure of reliability of a model. The higher Kappa, the better the model. The pattern of Kappa values of the models is the same as for the model accuracies: The classification tree model performed very bad; lasso, ridge and tuned support vector medium, and support vector very well. So again, the support vector model is to be preferred.","metadata":{}},{"cell_type":"code","source":"FitMeasures <- FitMeasures %>%\n        mutate(Kappa = c(confusion_tree$overall[[\"Kappa\"]], confusion_ridge$overall[[\"Kappa\"]], confusion_lasso$overall[[\"Kappa\"]],\n                         confusion_svm$overall[[\"Kappa\"]], confusion_e1017$overall[[\"Kappa\"]]))\n\n\n# Create the ggplot bar plot\nggplot(FitMeasures, aes(x = Kappa, y = Model)) +\n  geom_bar(stat = \"identity\", fill = ifelse(FitMeasures$Kappa == max(FitMeasures$Kappa), \"blue\", \"gray\")) +\n  geom_text(aes(label = round(Kappa, 3)), hjust = -0.1) +\n  labs(title = \"Test Set Kappa\", x = \"Kappa\") +\n  xlim(0, 1.1) +\n  theme_minimal() +\n  coord_flip()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Sensitivity\n\nSensitvitiy refers to the proportion of true positive cases among all actual positives, indicating how well a model detects true cases and avoids false negatives. We observed the exact same pattern as with the other fit measures and again decided on the support vector model. ","metadata":{}},{"cell_type":"code","source":"FitMeasures <- FitMeasures %>%\n                    mutate(Sensitivity = c(mean(confusion_tree$byClass[,\"Sensitivity\"]), mean(confusion_ridge$byClass[, \"Sensitivity\"]), mean(confusion_lasso$byClass[,\"Sensitivity\"]), \n                                           mean(confusion_svm$byClass[, \"Sensitivity\"]), mean(confusion_e1017$byClass[, \"Sensitivity\"])))\n\n\n# Create the ggplot bar plot\nggplot(FitMeasures, aes(x = Sensitivity, y = Model)) +\n  geom_bar(stat = \"identity\", fill = ifelse(FitMeasures$Sensitivity == max(FitMeasures$Sensitivity), \"blue\", \"gray\")) +\n  geom_text(aes(label = round(Sensitivity, 3)), hjust = -0.1) +\n  labs(title = \"Test Set Sensitivity\", x = \"Sensitivity\") +\n  xlim(0, 1.1) +\n  theme_minimal() +\n  coord_flip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Specificity\n\nSpecificity refers to the proportion of true negative cases among all actual negatives, indicating how well a model avoids \"false alarms\". Interestingly, there are no huge differences in specificity between the models. The model that again had a slight advantage over all other models was the support vector model.","metadata":{}},{"cell_type":"code","source":"FitMeasures <- FitMeasures %>%\n                    mutate(Specificity = c(mean(confusion_tree$byClass[,\"Specificity\"]), mean(confusion_ridge$byClass[, \"Specificity\"]), mean(confusion_lasso$byClass[,\"Specificity\"]), \n                                           mean(confusion_svm$byClass[, \"Specificity\"]), mean(confusion_e1017$byClass[, \"Specificity\"])))\n\n\n# Create the ggplot bar plot\nggplot(FitMeasures, aes(x = Specificity, y = Model)) +\n  geom_bar(stat = \"identity\", fill = ifelse(FitMeasures$Specificity == max(FitMeasures$Specificity), \"blue\", \"gray\")) +\n  geom_text(aes(label = round(Specificity, 3)), hjust = -0.1) +\n  labs(title = \"Test Set Specificity\", x = \"Specificity\") +\n  xlim(0, 1.1) +\n  theme_minimal() +\n  coord_flip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5. Final Model Choice\n\nTaking the bias-variance tradeoff, accuracy, sensitivity and specificity into account, we decided on the Support Vector Model as our final model. <br>","metadata":{}},{"cell_type":"markdown","source":"# 7. Submission\n\n## 7.1 Refit Support Vector Model on all data","metadata":{}},{"cell_type":"code","source":"# Read in the images as pixel values (discarding color channels)\nX = sapply(train_image_files, function(nm) c(readPNG(nm)[,,1])) %>% t() \ny = c(rep(\"anger\", length(anger)), rep(\"happy\", length(happy)), rep(\"sad\", length(sad)), rep(\"disgust\", length(disgust)))\n\nX_test = sapply(test_im, function(nm) c(readPNG(nm)[,,1])) %>% t() \n\n\n# Change row and column names of X to something more managable (caret::train requires column names)\nrownames(X)      = gsub(\".+x/\", \"\", rownames(X))\nrownames(X_test) = gsub(\".+x/\",  \"\", rownames(X_test))\n                \ncolnames(X) = colnames(X_test) = paste(\"p\",1:ncol(X), sep=\"\")\n\n# Extract Edge Features\nfinal_train_edge_features <- lapply(1:nrow(X), function(i) {\n  im <- matrix(X[i,], 48) \n  edges <- edge_detection(im)\n  return(flatten_edges(edges))\n})\nfinal_test_edge_features <- lapply(1:nrow(X_test), function(i) {\n  im <- matrix(X_test[i,], 48) \n  edges <- edge_detection(im)\n  return(flatten_edges(edges))\n})\n\n                \n# Extract Statistical Features\nfinal_train_stats_features <- stats(X, \"x\")\nfinal_test_stats_features <- stats(X_test, \"x\")\n                \nfinal_train_edge_features_matrix <- do.call(rbind, final_train_edge_features)\nfinal_test_edge_features_matrix <- do.call(rbind, final_test_edge_features)\n\nfinal_train_stats_features_edges <- stats(final_train_edge_features_matrix, \"x\")\nfinal_test_stats_features_edges <- stats(final_test_edge_features_matrix, \"x\")\n\n# Combine the datasets\nfinal_train_X_combined <- cbind(X, final_train_edge_features_matrix, final_train_stats_features, final_train_stats_features_edges)\nfinal_test_X_combined <- cbind(X_test, final_test_edge_features_matrix, final_test_stats_features, final_test_stats_features_edges)\n             \n# Speed up tuning by using all 4 CPU cores\ndoMC::registerDoMC(cores = 4)\n                \n# refit ... on all observations\nfinal_fit_svm <- train(x = final_train_X_combined, \n                 y = y, \n                 method = \"svmRadial\", \n                 trControl = trCntrl, \n                 preProcess = c(\"center\",\"scale\"), \n                 tuneLength = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The statistics of the new dataset (included the edges) are not computed succesfully.","metadata":{}},{"cell_type":"code","source":"# Combine all features\nfinal_train_X_with_edges <- cbind(X, train_edge_features)\nfinal_test_X_with_edges <- cbind(X_test, test_edge_features)\n\n# Extract Statistical Features\nfinal_train_stats_features <- stats(final_train_X_with_edges, \"x\")\nfinal_test_stats_features <- stats(final_test_X_with_edges, \"x\")\n\n# Combine all features\nfinal_train_X_combined <- cbind(X, do.call(rbind, train_edge_features), train_stats_features)\nfinal_test_X_combined <- cbind(X_test, do.call(rbind, test_edge_features), test_stats_features)\n                \n                \n# Extract Statistical Features\nfinal_train_stats_features <- stats(X, \"x\")\nfinal_test_stats_features <- stats(X_test, \"x\")\n\n                \n# Combine all features\nfinal_train_X_combined <- cbind(X, do.call(rbind, final_train_edge_features), final_train_stats_features)\nfinal_test_X_combined <- cbind(X_test, do.call(rbind, final_test_edge_features), final_test_stats_features)\n                \n                                \n                # refit ... on all observations\nfinal_fit_svm <- train(x = final_train_X_combined, \n                 y = y, \n                 method = \"svmRadial\", \n                 trControl = trCntrl, \n                 preProcess = c(\"center\",\"scale\"), \n                 tuneLength = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 Create Submission","metadata":{}},{"cell_type":"code","source":"## Make predictions\npred_svm = predict(final_fit_svm, final_test_X_combined, type='raw')\n\n## Write to file\ntibble(file = rownames(X_test), category = pred_svm) %>% \n    write_csv(path = \"submission.csv\")\n\n## Check result\ncat(readLines(\"submission.csv\",n=20), sep=\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Division of Labor\n\n* Sophia: Notebook, Model Fitting\n* Janne: Notebook, Model Fitting, Features\n* Leonie: Notebook, Features, Model Comparison","metadata":{}}]}